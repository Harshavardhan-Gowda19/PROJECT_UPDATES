{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":10744376,"sourceType":"datasetVersion","datasetId":6663123}],"dockerImageVersionId":30888,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\n\ndef load_image_dataset(base_dir, img_size=(224, 224)):\n    \"\"\"\n    Load images from subdirectories into a single dataset.\n    \n    Parameters:\n    base_dir (str): Path to the base directory containing class subdirectories\n    img_size (tuple): Target size for the images (default: (224, 224))\n    \n    Returns:\n    tuple: (X_data, y_data, label_map)\n    \"\"\"\n    \n    # Lists to store images and labels\n    images = []\n    labels = []\n    \n    # Create a mapping of directory names to numeric labels\n    label_map = {}\n    for idx, class_name in enumerate(sorted(os.listdir(base_dir))):\n        label_map[class_name] = idx\n    \n    # Load images and labels\n    for class_name in sorted(os.listdir(base_dir)):\n        class_dir = os.path.join(base_dir, class_name)\n        if not os.path.isdir(class_dir):\n            continue\n            \n        class_label = label_map[class_name]\n        \n        for img_name in os.listdir(class_dir):\n            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                img_path = os.path.join(class_dir, img_name)\n                try:\n                    # Load and preprocess image\n                    img = Image.open(img_path)\n                    img = img.convert('RGB')  # Convert to RGB format\n                    img = img.resize(img_size)  # Resize image\n                    img_array = np.array(img) / 255.0  # Normalize pixel values\n                    \n                    images.append(img_array)\n                    labels.append(class_label)\n                except Exception as e:\n                    print(f\"Error loading image {img_path}: {str(e)}\")\n    \n    # Convert lists to numpy arrays\n    X_data = np.array(images)\n    y_data = np.array(labels)\n    \n    return X_data, y_data, label_map\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:54:39.288459Z","iopub.execute_input":"2025-02-16T21:54:39.288792Z","iopub.status.idle":"2025-02-16T21:54:39.297965Z","shell.execute_reply.started":"2025-02-16T21:54:39.288765Z","shell.execute_reply":"2025-02-16T21:54:39.296911Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Example usage\nif __name__ == \"__main__\":\n    # Replace with your directory path\n    base_directory = \"/kaggle/input/traffic-data/traffic_Data/DATA\"\n    \n    # Load the dataset\n    X_data, y_data, label_map = load_image_dataset(\n        base_directory,\n        img_size=(224, 224)\n    )\n    \n    # Print dataset information\n    print(f\"Number of samples: {len(X_data)}\")\n    print(f\"Number of classes: {len(label_map)}\")\n    print(f\"Class mapping: {label_map}\")\n    \n    #Convert to TensorFlow dataset\n    dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:54:39.298936Z","iopub.execute_input":"2025-02-16T21:54:39.299183Z","iopub.status.idle":"2025-02-16T21:55:17.042228Z","shell.execute_reply.started":"2025-02-16T21:54:39.299161Z","shell.execute_reply":"2025-02-16T21:55:17.041031Z"}},"outputs":[{"name":"stdout","text":"Number of samples: 4170\nNumber of classes: 58\nClass mapping: {'0': 0, '1': 1, '10': 2, '11': 3, '12': 4, '13': 5, '14': 6, '15': 7, '16': 8, '17': 9, '18': 10, '19': 11, '2': 12, '20': 13, '21': 14, '22': 15, '23': 16, '24': 17, '25': 18, '26': 19, '27': 20, '28': 21, '29': 22, '3': 23, '30': 24, '31': 25, '32': 26, '33': 27, '34': 28, '35': 29, '36': 30, '37': 31, '38': 32, '39': 33, '4': 34, '40': 35, '41': 36, '42': 37, '43': 38, '44': 39, '45': 40, '46': 41, '47': 42, '48': 43, '49': 44, '5': 45, '50': 46, '51': 47, '52': 48, '53': 49, '54': 50, '55': 51, '56': 52, '57': 53, '6': 54, '7': 55, '8': 56, '9': 57}\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1739742912.642631      10 service.cc:148] XLA service 0x5741a0b2f040 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1739742912.642690      10 service.cc:156]   StreamExecutor device (0): TPU, 2a886c8\nI0000 00:00:1739742912.642694      10 service.cc:156]   StreamExecutor device (1): TPU, 2a886c8\nI0000 00:00:1739742912.642697      10 service.cc:156]   StreamExecutor device (2): TPU, 2a886c8\nI0000 00:00:1739742912.642700      10 service.cc:156]   StreamExecutor device (3): TPU, 2a886c8\nI0000 00:00:1739742912.642703      10 service.cc:156]   StreamExecutor device (4): TPU, 2a886c8\nI0000 00:00:1739742912.642705      10 service.cc:156]   StreamExecutor device (5): TPU, 2a886c8\nI0000 00:00:1739742912.642709      10 service.cc:156]   StreamExecutor device (6): TPU, 2a886c8\nI0000 00:00:1739742912.642711      10 service.cc:156]   StreamExecutor device (7): TPU, 2a886c8\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import cv2\n\ndef extract_edge_features(X_data):\n    features_1 = []\n\n    for img_array in X_data:\n        # Convert the image array to uint8 before processing\n        img = (np.array(img_array) * 255).astype(np.uint8)\n        \n        # Convert to grayscale using PIL (more efficient)\n        gray_image = Image.fromarray(img).convert(\"L\")\n\n        # Convert grayscale image back to NumPy array for OpenCV processing\n        gray_array = np.array(gray_image)\n\n        # Apply the Sobel operator in the X direction\n        sobel_x = cv2.Sobel(gray_array, cv2.CV_64F, 1, 0, ksize=3)\n\n        # Apply the Sobel operator in the Y direction\n        sobel_y = cv2.Sobel(gray_array, cv2.CV_64F, 0, 1, ksize=3)\n\n        # Calculate the magnitude of the gradients\n        magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\n\n        # Normalize to 0-255, avoid division by zero\n        max_magnitude = np.max(magnitude)\n        if max_magnitude > 0:\n            magnitude = np.uint8(255 * magnitude / max_magnitude)\n        else:\n            magnitude = np.uint8(magnitude)  # If max is 0, no edges exist\n\n        # Store the processed image in features_1\n        features_1.append(magnitude)\n\n    return np.array(features_1)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:55:17.043400Z","iopub.execute_input":"2025-02-16T21:55:17.043660Z","iopub.status.idle":"2025-02-16T21:55:17.050612Z","shell.execute_reply.started":"2025-02-16T21:55:17.043622Z","shell.execute_reply":"2025-02-16T21:55:17.049657Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def extract_line_features(X_data):\n    features_2 = []\n\n    for img_array in X_data:\n        # Convert the image array to uint8 before processing\n        img = (np.array(img_array) * 255).astype(np.uint8)\n        \n        # Convert to grayscale using PIL\n        gray_image = Image.fromarray(img).convert(\"L\")\n\n        # Convert grayscale image back to NumPy array for OpenCV processing\n        gray_array = np.array(gray_image)\n\n        # Find edges using Canny edge detector\n        edges = cv2.Canny(gray_array, 50, 200)\n\n        # Detect lines using Hough Line Transform\n        lines = cv2.HoughLinesP(edges, 1, np.pi/180, 68, minLineLength=15, maxLineGap=250)\n\n        # Convert grayscale image to BGR (for colored line drawing)\n        line_image = cv2.cvtColor(gray_array, cv2.COLOR_GRAY2BGR)\n\n        # Draw detected lines on the image\n        if lines is not None:\n            for line in lines:\n                x1, y1, x2, y2 = line[0]\n                cv2.line(line_image, (x1, y1), (x2, y2), (255, 0, 0), 3)  # Blue lines\n\n        # Store the processed image in features_2\n        features_2.append(line_image)\n\n    return np.array(features_2) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:55:17.051872Z","iopub.execute_input":"2025-02-16T21:55:17.052112Z","iopub.status.idle":"2025-02-16T21:55:17.065473Z","shell.execute_reply.started":"2025-02-16T21:55:17.052088Z","shell.execute_reply":"2025-02-16T21:55:17.064320Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def extract_contour_features(X_data):\n    features_3 = []\n\n    for img_array in X_data:\n        # Convert the image array to uint8 before processing\n        img = (np.array(img_array) * 255).astype(np.uint8)\n        \n        # Convert to grayscale using PIL\n        gray_image = Image.fromarray(img).convert(\"L\")\n\n        # Convert grayscale image back to NumPy array for OpenCV processing\n        gray_array = np.array(gray_image)\n\n        # STEP 1: Apply GaussianBlur to reduce noise\n        blurred = cv2.GaussianBlur(gray_array, (5, 5), 0)\n\n        # STEP 2: Use Canny edge detector\n        edges = cv2.Canny(blurred, 50, 150)\n\n        # STEP 3: Find contours\n        contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Create a color copy of the image for contour drawing (in BGR format)\n        contour_image = cv2.cvtColor(gray_array, cv2.COLOR_GRAY2BGR)\n\n        # Draw contours in green (0, 255, 0) with thickness of 1\n        cv2.drawContours(contour_image, contours, -1, (0, 255, 0), 1)\n\n        # Store the processed contour image in features_3\n        features_3.append(contour_image)\n\n    return np.array(features_3)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:55:17.066442Z","iopub.execute_input":"2025-02-16T21:55:17.066706Z","iopub.status.idle":"2025-02-16T21:55:17.132536Z","shell.execute_reply.started":"2025-02-16T21:55:17.066681Z","shell.execute_reply":"2025-02-16T21:55:17.131578Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"features_1 = extract_edge_features(X_data)\nfeatures_2 = extract_line_features(X_data)\nfeatures_3 = extract_contour_features(X_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:55:17.133846Z","iopub.execute_input":"2025-02-16T21:55:17.134084Z","iopub.status.idle":"2025-02-16T21:55:42.257368Z","shell.execute_reply.started":"2025-02-16T21:55:17.134063Z","shell.execute_reply":"2025-02-16T21:55:42.256193Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"if features_2.shape[-1] == 3:  # Check if last dimension is 3 (RGB)\n    features_2_new = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in features_2])\nelse:\n    print(\"Already grayscale, skipping conversion!\")\n    features_2_new = features_2  # No need to convert\n\nprint(features_2_new.shape)  # Should be (4170, 224, 224)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:55:42.258378Z","iopub.execute_input":"2025-02-16T21:55:42.258660Z","iopub.status.idle":"2025-02-16T21:55:42.498473Z","shell.execute_reply.started":"2025-02-16T21:55:42.258615Z","shell.execute_reply":"2025-02-16T21:55:42.497026Z"}},"outputs":[{"name":"stdout","text":"(4170, 224, 224)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"if features_3.shape[-1] == 3:  # Check if last dimension is 3 (RGB)\n    features_3_new = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in features_3])\nelse:\n    print(\"Already grayscale, skipping conversion!\")\n    features_3_new = features_3  # No need to convert\n\nprint(features_3_new.shape)  # Should be (4170, 224, 224)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:55:42.499192Z","iopub.execute_input":"2025-02-16T21:55:42.499442Z","iopub.status.idle":"2025-02-16T21:55:42.709316Z","shell.execute_reply.started":"2025-02-16T21:55:42.499417Z","shell.execute_reply":"2025-02-16T21:55:42.708189Z"}},"outputs":[{"name":"stdout","text":"(4170, 224, 224)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import numpy as np\n\ndef preprocess_and_concatenate_features(features_1,features_2, features_3):\n    \n    # Get the number of samples\n    n_samples = features_3.shape[0]\n    \n    # Reshape 3D feature (4170, 224, 224) -> (4170, 224*224)\n    feature_1_reshaped = features_3.reshape(n_samples, -1)\n    \n    # Reshape 4D feature (4170, 224, 224, 3) -> (4170, 224*224*3)\n    feature_2_reshaped = features_2.reshape(n_samples, -1)\n\n    feature_3_reshaped = features_3.reshape(n_samples, -1)\n    \n    # Concatenate along feature axis\n    concatenated_features = np.concatenate(\n        (feature_1_reshaped, feature_2_reshaped,feature_3_reshaped), \n        axis=1\n    )\n    \n    print(f\"Original 3D feature shape: {features_1.shape}\")\n    print(f\"Original 4D feature shape: {features_2.shape}\")\n    print(f\"Original 4D feature shape: {features_3.shape}\")\n    print(f\"Reshaped 3D feature shape: {feature_1_reshaped.shape}\")\n    print(f\"Reshaped 4D feature shape: {feature_2_reshaped.shape}\")\n    print(f\"Reshaped 4D feature shape: {feature_3_reshaped.shape}\")\n    print(f\"Final concatenated shape: {concatenated_features.shape}\")\n    \n    return concatenated_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:55:42.710186Z","iopub.execute_input":"2025-02-16T21:55:42.710489Z","iopub.status.idle":"2025-02-16T21:55:42.716057Z","shell.execute_reply.started":"2025-02-16T21:55:42.710467Z","shell.execute_reply":"2025-02-16T21:55:42.715272Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"concatenated = preprocess_and_concatenate_features(features_1, features_2_new, features_3_new)\nnp.shape(concatenated)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:55:42.716934Z","iopub.execute_input":"2025-02-16T21:55:42.717202Z","iopub.status.idle":"2025-02-16T21:55:42.950773Z","shell.execute_reply.started":"2025-02-16T21:55:42.717178Z","shell.execute_reply":"2025-02-16T21:55:42.950095Z"}},"outputs":[{"name":"stdout","text":"Original 3D feature shape: (4170, 224, 224)\nOriginal 4D feature shape: (4170, 224, 224)\nOriginal 4D feature shape: (4170, 224, 224)\nReshaped 3D feature shape: (4170, 50176)\nReshaped 4D feature shape: (4170, 50176)\nReshaped 4D feature shape: (4170, 50176)\nFinal concatenated shape: (4170, 150528)\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(4170, 150528)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=500)  # Reduce to 500 dimensions\nX_pca = pca.fit_transform(concatenated)\nnp.shape(X_pca)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:55:42.951776Z","iopub.execute_input":"2025-02-16T21:55:42.952001Z","iopub.status.idle":"2025-02-16T21:56:13.490154Z","shell.execute_reply.started":"2025-02-16T21:55:42.951979Z","shell.execute_reply":"2025-02-16T21:56:13.489041Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(4170, 500)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_pca, y_data, test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:56:13.491237Z","iopub.execute_input":"2025-02-16T21:56:13.491903Z","iopub.status.idle":"2025-02-16T21:56:13.501906Z","shell.execute_reply.started":"2025-02-16T21:56:13.491872Z","shell.execute_reply":"2025-02-16T21:56:13.500380Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', cache_size=7000)  # Increase cache size\nsvm_rbf.fit(X_train_new, y_train_new)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:56:13.502722Z","iopub.execute_input":"2025-02-16T21:56:13.502933Z","iopub.status.idle":"2025-02-16T21:56:15.382869Z","shell.execute_reply.started":"2025-02-16T21:56:13.502913Z","shell.execute_reply":"2025-02-16T21:56:15.381212Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"SVC(cache_size=7000)","text/html":"<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(cache_size=7000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(cache_size=7000)</pre></div> </div></div></div></div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"#with dimensionality reduction \naccuracy = svm_rbf.score(X_test_new, y_test_new)\nprint(f\"Test Accuracy: {accuracy:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:56:15.383825Z","iopub.execute_input":"2025-02-16T21:56:15.384097Z","iopub.status.idle":"2025-02-16T21:56:16.408549Z","shell.execute_reply.started":"2025-02-16T21:56:15.384072Z","shell.execute_reply":"2025-02-16T21:56:16.406853Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.88\n","output_type":"stream"}],"execution_count":17}]}